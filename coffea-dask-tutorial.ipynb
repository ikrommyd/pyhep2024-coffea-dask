{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Distributed Columnar HEP analysis using coffea + dask\n",
    "\n",
    "### Warning: In this talk we will assume some familiarity with the uproot+awkward way of dealing with data.\n",
    "\n",
    "### Columnar analysis and coffea\n",
    "What is `coffea`? Coffea is a set of basic tools and wrappers for enabling not-too-alien syntax when running columnar Collider HEP analysis.\n",
    "\n",
    "What is columnar analysis:\n",
    "\n",
    "* Event loop analysis:\n",
    "    - Load relevant values for a specific event into local variables\n",
    "    - Evaluate several expressions\n",
    "    - Store derived values\n",
    "    - Repeat (explicit outer loop)\n",
    "* Columnar analysis:\n",
    "    - Load relevant values for many events into contiguous arrays\n",
    "    - Evaluate several array programming expressions\n",
    "        - Implicit inner loops\n",
    "        - Plan analysis by composing data manipulations\n",
    "    - Store derived values\n",
    "\n",
    "<img src=\"img/columnar.png\" width=\"400\" style=\"margin-right: 20px;\"><img src=\"img/columnar-code.png\" width=\"400\">\n",
    "\n",
    "### Dask\n",
    "\n",
    "Dask provides an interface for specifying/locating input data and then describing manipulations on that data are organized into a task graph. This task graph can then be executed on local compute or on a cluster. Dask Array and Dask Dataframe deal well with rectangular data. They provide a scalable interface to describe manipulations of data that may not fit into\n",
    "system memory by mapping transformations onto partitions of the data that fit in memory.\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-overview.svg\" width=\"400\" style=\"margin-right: 20px;\"><img src=\"https://docs.dask.org/en/latest/_images/dask-array.svg\" width=\"400\">\n",
    "\n",
    "But in physics we're dealing with [jagged or ragged arrays](https://en.wikipedia.org/wiki/Jagged_array). A ragged array is something like this:\n",
    "\n",
    "```\n",
    "[[1, 2, 3],\n",
    " [4],\n",
    " [],\n",
    " [5, 6]]\n",
    "---------------------\n",
    "type: 4 * var * int64\n",
    "```\n",
    "\n",
    "In the pythonic HEP ecosystem, we deal with those kinds of arrays using [awkward](https://github.com/scikit-hep/awkward). Awkward Arrays are general tree-like data structures, like JSON, but contiguous in memory and operated upon with compiled, vectorized code like NumPy. For more information, please visit the [awkward array docs](https://awkward-array.org/doc/main/index.html) and/or see [previous talks from Jim Pivarski](https://github.com/jpivarski-talks/).\n",
    "\n",
    "### How jagged arrays and histogramming are integrated with dask: awkward array 2.0, dask_awkward, dask_histogram, and coffea\n",
    "\n",
    "<img src=\"img/coffea-upgrade.png\" width=\"600\">\n",
    "\n",
    "Awkward array 2.0 features an improved and streamlined backend with. The backend is using only C and python without any C++ metadata handling. `ak.virtual` delayed computations are replaced by [dask-awkward](https://github.com/dask-contrib/dask-awkward).\n",
    "\n",
    "`dask_awkward` and `dask_histogram` bring delayed, distributed computation to `awkward array 2.0` based analyses and libraries. [Uproot](https://github.com/scikit-hep/uproot5) now provides lazy reading of data via [uproot.dask](https://uproot.readthedocs.io/en/latest/uproot._dask.dask.html).\n",
    "\n",
    "#### Our partitions are split on the event axis since each event is independent and we never run computations that combine more than 1 events.\n",
    "\n",
    "All that provides access to dask at all layers of analysis which yields improved parallelism and better factorization away from compute infrastructure.\n",
    "Coffea, and `NanoEvents` in particular which was almost entirely based on `ak.virtual`, now leverage all this infrastructure.\n",
    "\n",
    "Please see [Lindsey's CHEP 2023 talk](https://indico.jlab.org/event/459/contributions/11533/attachments/9496/13762/CoffeaCHEP_LindseyGray_09052023.pdf) for more info on those upgrades and also see the most recent talks from [Lindsey](https://indico.cern.ch/event/1383972/contributions/5825304/) and [Jim](https://github.com/jpivarski-talks/2023-07-24-tac-hep-tutorial/blob/main/horizontal.ipynb).\n",
    "\n",
    "Let's start showcasing how to use all that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist\n",
    "import dask\n",
    "import awkward as ak\n",
    "import hist.dask as hda\n",
    "import dask_awkward as dak\n",
    "import uproot\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea.dataset_tools import apply_to_fileset, preprocess\n",
    "from distributed import Client\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We read our data with `NanoEventsFactory`. For more info about how NanoEvents constructs the events array please read the [coffea docs](https://coffeateam.github.io/coffea/) and [Nick's PyHEP 2020 talk](https://indico.cern.ch/event/882824/contributions/3927107/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = NanoEventsFactory.from_root({\"data/DYto2E.root\": \"Events\"}).events()\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events.Electron.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Events is a lazy dask-awkward collection. In order to actually compute something, you'd have to call `.compute()` on some collection. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electron.pt.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's do a Tag & Probe analysis to measure some ID efficiency to showcase the functionality of coffea. Tag & Probe is used as a technique because we cannot trust our simulation to perfectly describe the detector response so we need to measure the efficiency in data, categorizing signal and background by inference. How do we do this without selection bias? Thanks to the Z boson, we have a solution: tag one electron in an event that passes some high quality requirement, then look for a \"probe\" electron such that the invariant mass of the di-electron pair matches that of the Z boson. If we count the number of probes falling in some mass window that pass and fail our ID, we can infer the true electron tagging efficiency in data, despite not knowing the ground truth for each electron. This can also be used for photons instead of electrons as electrons will interact like photons with our calorimeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_match(leptons, trigobjs, pdgid, pt, filterbit):\n",
    "    \"\"\"\n",
    "    Matches leptons with online trigger objects using dR < 0.1 criterion\n",
    "    The filterbit corresponds to the trigger we want our leptons to have fired\n",
    "    \"\"\"\n",
    "    pass_pt = trigobjs.pt > pt\n",
    "    pass_id = abs(trigobjs.id) == pdgid\n",
    "    pass_filterbit = (trigobjs.filterBits & (0x1 << filterbit)) != 0\n",
    "    trigger_cands = trigobjs[pass_pt & pass_id & pass_filterbit]\n",
    "    delta_r = leptons.metric_table(trigger_cands)\n",
    "    pass_delta_r = delta_r < 0.1\n",
    "    n_of_trigger_matches = dak.sum(pass_delta_r, axis=2)\n",
    "    trig_matched_locs = n_of_trigger_matches >= 1\n",
    "\n",
    "    return trig_matched_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We want to select diphotons in our case. The function that will help us do this is `ak.cartesian` on `axis=1` (the default).\n",
    "This operation is equivalent to this C++ `for` loop over two collections:\n",
    "\n",
    "```c++\n",
    "for (int i = 0; i < numbers.size(); i++) {\n",
    "  for (int j = 0; j < letters.size(); j++) {\n",
    "    // compute formula with numbers[i] and letters[j]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "and this is a cartoon of the operation:\n",
    "\n",
    "<img src=\"img/cartoon-cartesian.png\" width=\"300\">\n",
    "\n",
    "It creates the cartesian product between two collections.\n",
    "However, we will need to drop pairs of photons with themselves.\n",
    "\n",
    "We could also have used `ak.combinations` on `axis=1` (the default).\n",
    "This operation is equivalent to this C++ `for` loop over one collection:\n",
    "\n",
    "```c++\n",
    "for (int i = 0; i < numbers.size(); i++) {\n",
    "  for (int j = i + 1; i < numbers.size(); j++) {\n",
    "    // compute formula with numbers[i] and numbers[j]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "and this is a cartoon of the operation:\n",
    "\n",
    "<img src=\"img/cartoon-combinations.png\" width=\"300\">\n",
    "\n",
    "It finds all pairs within a single collection, without repetition. If we had used that, we would need to do it twice and concatenate because each photon can be both a tag and a probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select events where Ele30_WPTight_Gsf (an isolated electron with a transverse momentum 𝑝_T > 30 GeV)\n",
    "good_events = events[events.HLT.Ele30_WPTight_Gsf]\n",
    "\n",
    "# form all possible diphoton pairs and removing pairs of photons with themselves\n",
    "ij = dak.argcartesian([good_events.Photon, good_events.Photon])\n",
    "is_not_diag = ij[\"0\"] != ij[\"1\"]\n",
    "i, j = dak.unzip(ij[is_not_diag])\n",
    "zcands = dak.zip({\"tag\": good_events.Photon[i], \"probe\": good_events.Photon[j]})\n",
    "\n",
    "# request tags to have tight cutbased id and mask zcands\n",
    "pass_tight_id_tags = zcands.tag.cutBased >= 3\n",
    "zcands = zcands[pass_tight_id_tags]\n",
    "\n",
    "# do not allow tags to be in the ECAL transition region between barrel and endcap\n",
    "pass_eta_ebeegap_tags = (abs(zcands.tag.eta) < 1.4442) | (abs(zcands.tag.eta) > 1.566)\n",
    "zcands = zcands[pass_eta_ebeegap_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigobjs = good_events.TrigObj\n",
    "\n",
    "# put pt and eta cuts on the tags and probes\n",
    "pt_cond_tags = zcands.tag.pt > 35\n",
    "pt_cond_probes = zcands.probe.pt > 5\n",
    "eta_cond_tags = abs(zcands.tag.eta) < 2.17\n",
    "eta_cond_probes = abs(zcands.probe.eta) < 2.5\n",
    "\n",
    "# request the tag to \"be an electron and not a photon\" by requiring a pixel track and an associated electron index.\n",
    "has_matched_electron_tags = (zcands.tag.electronIdx != -1) & (zcands.tag.pixelSeed)\n",
    "\n",
    "# require tags to have fired Ele30_WPTight_Gsf\n",
    "# pdgid = 11 for electron, pt = 30 threshold and filterbit = 1 for this trigger\n",
    "trig_matched_tag = trigger_match(zcands.tag.matched_electron, trigobjs, 11, 30, 1)\n",
    "\n",
    "# mask Z-candidates with all previous requirements and cuts\n",
    "zcands = zcands[\n",
    "    has_matched_electron_tags\n",
    "    & trig_matched_tag\n",
    "    & pt_cond_tags\n",
    "    & pt_cond_probes\n",
    "    & eta_cond_tags\n",
    "    & eta_cond_probes\n",
    "]\n",
    "\n",
    "# drop every event that doesn't have a good tag\n",
    "events_with_tags = ak.num(zcands.tag, axis=1) >= 1\n",
    "zcands = zcands[events_with_tags]\n",
    "trigobjs = trigobjs[events_with_tags]\n",
    "good_events = good_events[events_with_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only allow invariant mass of tag-probe pairs between 50 and 130 GeV\n",
    "mass = (zcands.tag + zcands.probe).mass\n",
    "in_mass_window = (mass > 50) & (mass < 130)\n",
    "zcands = zcands[in_mass_window]\n",
    "\n",
    "# we will be checking the efficiency of Tight Cutbased ID (that is 3 in NanoAOD for photons)\n",
    "# therefore a passing probe is a probe that passed this ID requirement\n",
    "is_passing_probe = zcands.probe.cutBased >= 3\n",
    "passing_pairs = zcands[is_passing_probe]\n",
    "failing_pairs = zcands[~is_passing_probe]\n",
    "\n",
    "# calculate the invariant mass of the passing/failing tag-probe pairs and flatten\n",
    "passing_pairs_mass = dak.flatten((passing_pairs.tag + passing_pairs.probe).mass)\n",
    "failing_pairs_mass = dak.flatten((failing_pairs.tag + failing_pairs.probe).mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Up until this point, nothing has been computed. What has happened is that the operations have only been run on typetracer arrays and a task graph which you can inspect has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(passing_pairs_mass, optimize_graph=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "It's also worth noting at this point that you can lazily write arrays/events to root files using `uproot.dask_write`. We can also use `.repartition()` to change the number of partitions of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "uproot.dask_write(\n",
    "    passing_pairs.repartition(3),\n",
    "    \"output/\",\n",
    "    prefix=\"passing-pairs\",\n",
    "    tree_name=\"mytree\",\n",
    "    compute=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_passing_pairs = uproot.dask(\n",
    "    [\n",
    "        \"output/passing-pairs-part0.root\",\n",
    "        \"output/passing-pairs-part1.root\",\n",
    "        \"output/passing-pairs-part2.root\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_passing_pairs.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_passing_pairs.probe_pt.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Actually this isn't the spot where we have to evaluate this task graph. We can do lazy histograms as well thanks to `dask-histogram` and its minimal interface with `hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill histograms with the flattened invariant mass arrays\n",
    "hpass = hda.Hist(hist.axis.Regular(80, 50, 130, label=\"mll\"))\n",
    "hfail = hda.Hist(hist.axis.Regular(80, 50, 130, label=\"mll\"))\n",
    "\n",
    "hpass.fill(passing_pairs_mass)\n",
    "hfail.fill(failing_pairs_mass)\n",
    "print(hpass.staged_fills())\n",
    "print(hfail.staged_fills())\n",
    "hpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now the histogramming has become part of the task-graph as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(hpass, optimize_graph=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(hpass, optimize_graph=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "What one can also do is view the necessary columns that task graphs need to read from the files in order to compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dak.necessary_columns(hpass, hfail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Only the branches specified above will actually be read from the file. This minimizes data reading. For more info you can read the [dask-awkward docs](https://dask-awkward.readthedocs.io/en/stable/more/optimization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Let's now plot the invariant mass spectra of the passing and failing probes. We first gotta call `.compute()` on our histograms and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axp, axf) = plt.subplots(1, 2, sharey=True, figsize=(12, 6))\n",
    "\n",
    "hpass.compute().plot(ax=axp, yerr=False)\n",
    "axp.set_title(\"Passing probes\")\n",
    "axp.set_ylabel(\"Events\")\n",
    "axp.set_xlabel(\"Dielectron mass [GeV]\")\n",
    "\n",
    "hfail.compute().plot(ax=axf, yerr=False)\n",
    "axf.set_title(\"Failing probes\")\n",
    "axf.set_xlabel(\"Dielectron mass [GeV]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "And we got our histograms of the passing and failing probes. They don't look great because we started only from 1000 events.\n",
    "Here one would fit a Signal + Background model to the passing and failing probes to extract the actual number of passing and failing probes coming from a Z boson since there is background in the invariant mass spectra. Assuming that from the fit the passing and failing probe yields from the Z are $n_{pass}$ and $n_{fail}$ respectively, then the efficiency is\n",
    "\n",
    "$$\n",
    "\\epsilon = \\frac{n_{pass}}{n_{pass} + n_{fail}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Up until this point, we've been doing a lot of things more manually. We manually read our events, ran the analysis code and called `.compute()` on each of the collections we wanted to actually evaluate in the end. We've been very explicit for demonstration purposes but this isn't the most efficient way of dealing with things.\n",
    "First of all, you'd normally want to use `dask.compute(**things)` so that dask optimizes and fuses the separate task graphs of \"things\" as much as it can. Secondly, we been computing things locally using local threads in our previous example. Normally one would use a local or distributed dask [Cluster](https://docs.dask.org/en/stable/deploying.html) and [Client](https://distributed.dask.org/en/latest/client.html). Let's do that while also combining it with more of coffea's advanced features in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "We start be defining our fileset and using `coffea`'s `preprocess` function to preprocess our fileset.\n",
    "This checks whether the files are accessible, reads things like the number of events contained within the files and splits them into partitions of ~100000 events each.\n",
    "It will return a \"runnable\" fileset and an \"updated\" fileset with the partitions specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileset = {\n",
    "#     \"DoubleMuon\": {\n",
    "#         \"files\": {\n",
    "#             \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\": \"Events\",\n",
    "#             \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root\": \"Events\",\n",
    "#             \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/not_a_DoubleMu_file.root\": \"Events\",\n",
    "#         }\n",
    "#     },\n",
    "#     \"ZZto4mu\": {\n",
    "#         \"files\": {\n",
    "#             \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root\": \"Events\",\n",
    "#             \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/not_a_ZZTo4mu_file.root\": \"Events\",\n",
    "#         }\n",
    "#     },\n",
    "# }\n",
    "\n",
    "fileset = {\n",
    "    \"ZZto4mu\": {\n",
    "        \"files\": {\n",
    "            \"data/ZZTo4mu.root\": \"Events\",\n",
    "            \"data/not_a_ZZTo4mu_file.root\": \"Events\",\n",
    "        }\n",
    "    },\n",
    "    \"SMHiggsToZZTo4L\": {\n",
    "        \"files\": {\n",
    "            \"data/SMHiggsToZZTo4L.root\": \"Events\",\n",
    "            \"data/not_a_SMHiggsToZZTo4L_file.root\": \"Events\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "dataset_runnable, dataset_updated = preprocess(\n",
    "    fileset,\n",
    "    step_size=100_000,\n",
    "    skip_bad_files=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "A coffea concept that is actually not required to be used anymore is the [coffea processor](https://coffeateam.github.io/coffea/concepts.html#coffea-processor). This is very useful for organization but one could also use any function `function(events)` that defines their analysis. We need this signature because this is what coffea requires to apply the analysis to the whole fileset. If a processor is used, then the processor's `.process(events)` method will be the one applied to the fileset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Let's write up a fancy 4-muon analysis, searching for diboson events. The following processor showcases a lot of tricky things for beginners, like using awkward's array builder, jit compilation, using `dak.map_partitions` to apply functions that are not written in plain awkward to partitions and defining a custom vector collection. One such example used in the `coffea` code is [this](https://github.com/CoffeaTeam/coffea/blob/225dc995d18af092d0c1969edae4e03fcb6c88e8/src/coffea/lumi_tools/lumi_tools.py#L173).\n",
    "Notice that we use `ak.some_function` instead of `dak.some_function` as there is an automatic dispatch from awkward -> dask-awkward when a function is called on a dask-awkward collection.\n",
    "We will use `BaseSchema` to showcase the vector building but we could have used `NanoAODSchema` and that wouldn't be required. Please try and read the processor and understand what it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def find_4lep_kernel(events_leptons, builder):\n",
    "    \"\"\"Search for valid 4-lepton combinations from an array of events * leptons {charge, ...}\n",
    "\n",
    "    A valid candidate has two pairs of leptons that each have balanced charge\n",
    "    Outputs an array of events * candidates {indices 0..3} corresponding to all valid\n",
    "    permutations of all valid combinations of unique leptons in each event\n",
    "    (omitting permutations of the pairs)\n",
    "    \"\"\"\n",
    "    for leptons in events_leptons:\n",
    "        builder.begin_list()\n",
    "        nlep = len(leptons)\n",
    "        for i0 in range(nlep):\n",
    "            for i1 in range(i0 + 1, nlep):\n",
    "                if leptons[i0].charge + leptons[i1].charge != 0:\n",
    "                    continue\n",
    "                for i2 in range(nlep):\n",
    "                    for i3 in range(i2 + 1, nlep):\n",
    "                        if len({i0, i1, i2, i3}) < 4:\n",
    "                            continue\n",
    "                        if leptons[i2].charge + leptons[i3].charge != 0:\n",
    "                            continue\n",
    "                        builder.begin_tuple(4)\n",
    "                        builder.index(0).integer(i0)\n",
    "                        builder.index(1).integer(i1)\n",
    "                        builder.index(2).integer(i2)\n",
    "                        builder.index(3).integer(i3)\n",
    "                        builder.end_tuple()\n",
    "        builder.end_list()\n",
    "\n",
    "    return builder\n",
    "\n",
    "\n",
    "def find_4lep(events_leptons):\n",
    "    if ak.backend(events_leptons) == \"typetracer\":\n",
    "        # here we fake the output of find_4lep_kernel since\n",
    "        # operating on length-zero data returns the wrong layout!\n",
    "        ak.typetracer.touch_data(\n",
    "            events_leptons.charge\n",
    "        )  # force touching of the necessary data\n",
    "        # Use x = ak.typetracer.length_zero_if_typetracer(events_leptons.charge) or x = ak.typetracer.length_one_if_typetracer(events_leptons.charge)\n",
    "        # if you want a a length-zero/one NumPy-backed array to be returned for your computations\n",
    "        return ak.Array(\n",
    "            ak.Array([[(0, 0, 0, 0)]]).layout.to_typetracer(forget_length=True)\n",
    "        )\n",
    "    return find_4lep_kernel(events_leptons, ak.ArrayBuilder()).snapshot()\n",
    "\n",
    "\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    # The processor can also have an __init__ method\n",
    "\n",
    "    def process(self, events):\n",
    "        dataset_axis = hist.axis.StrCategory(\n",
    "            [], growth=True, name=\"dataset\", label=\"Primary dataset\"\n",
    "        )\n",
    "        mass_axis = hist.axis.Regular(\n",
    "            300, 0, 300, name=\"mass\", label=r\"$m_{\\mu\\mu}$ [GeV]\"\n",
    "        )\n",
    "        pt_axis = hist.axis.Regular(300, 0, 300, name=\"pt\", label=r\"$p_{T,\\mu}$ [GeV]\")\n",
    "\n",
    "        h_nMuons = hda.Hist(\n",
    "            dataset_axis,\n",
    "            hda.hist.hist.axis.IntCategory(\n",
    "                range(6), name=\"nMuons\", label=\"Number of good muons\"\n",
    "            ),\n",
    "            storage=\"weight\",\n",
    "            label=\"Counts\",\n",
    "        )\n",
    "        h_m4mu = hda.hist.Hist(\n",
    "            dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\"\n",
    "        )\n",
    "        h_mZ1 = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_mZ2 = hda.hist.Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_ptZ1mu1 = hda.hist.Hist(\n",
    "            dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\"\n",
    "        )\n",
    "        h_ptZ1mu2 = hda.hist.Hist(\n",
    "            dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\"\n",
    "        )\n",
    "\n",
    "        cutflow = dict()\n",
    "\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        muons = ak.zip(\n",
    "            {\n",
    "                \"pt\": events.Muon_pt,\n",
    "                \"eta\": events.Muon_eta,\n",
    "                \"phi\": events.Muon_phi,\n",
    "                \"mass\": events.Muon_mass,\n",
    "                \"charge\": events.Muon_charge,\n",
    "                \"isolation\": events.Muon_pfRelIso03_all,\n",
    "            },\n",
    "            with_name=\"PtEtaPhiMCandidate\",\n",
    "            behavior=candidate.behavior,\n",
    "        )\n",
    "\n",
    "        # make sure they are sorted by transverse momentum\n",
    "        muons = muons[ak.argsort(muons.pt, axis=1)]\n",
    "\n",
    "        cutflow[\"all events\"] = ak.num(muons, axis=0)\n",
    "\n",
    "        # impose some quality and minimum pt cuts on the muons\n",
    "        muons = muons[(muons.pt > 5) & (muons.isolation < 0.2)]\n",
    "        cutflow[\"at least 4 good muons\"] = ak.sum(ak.num(muons) >= 4)\n",
    "        h_nMuons.fill(dataset=dataset, nMuons=ak.num(muons))\n",
    "\n",
    "        # reduce first axis: skip events without enough muons\n",
    "        muons = muons[ak.num(muons) >= 4]\n",
    "\n",
    "        # find all candidates with helper function\n",
    "        fourmuon = dak.map_partitions(find_4lep, muons)\n",
    "        fourmuon = [muons[fourmuon[idx]] for idx in \"0123\"]\n",
    "\n",
    "        fourmuon = ak.zip(\n",
    "            {\n",
    "                \"z1\": ak.zip(\n",
    "                    {\n",
    "                        \"lep1\": fourmuon[0],\n",
    "                        \"lep2\": fourmuon[1],\n",
    "                        \"p4\": fourmuon[0] + fourmuon[1],\n",
    "                    }\n",
    "                ),\n",
    "                \"z2\": ak.zip(\n",
    "                    {\n",
    "                        \"lep1\": fourmuon[2],\n",
    "                        \"lep2\": fourmuon[3],\n",
    "                        \"p4\": fourmuon[2] + fourmuon[3],\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        cutflow[\"at least one candidate\"] = ak.sum(ak.num(fourmuon) > 0)\n",
    "\n",
    "        # require minimum dimuon mass\n",
    "        fourmuon = fourmuon[(fourmuon.z1.p4.mass > 60.0) & (fourmuon.z2.p4.mass > 20.0)]\n",
    "        cutflow[\"minimum dimuon mass\"] = ak.sum(ak.num(fourmuon) > 0)\n",
    "\n",
    "        # choose permutation with z1 mass closest to nominal Z boson mass\n",
    "        bestz1 = ak.singletons(ak.argmin(abs(fourmuon.z1.p4.mass - 91.1876), axis=1))\n",
    "        fourmuon = ak.flatten(fourmuon[bestz1])\n",
    "\n",
    "        h_m4mu.fill(\n",
    "            dataset=dataset,\n",
    "            mass=(fourmuon.z1.p4 + fourmuon.z2.p4).mass,\n",
    "        )\n",
    "        h_mZ1.fill(\n",
    "            dataset=dataset,\n",
    "            mass=fourmuon.z1.p4.mass,\n",
    "        )\n",
    "        h_mZ2.fill(\n",
    "            dataset=dataset,\n",
    "            mass=fourmuon.z2.p4.mass,\n",
    "        )\n",
    "        h_ptZ1mu1.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep1.pt,\n",
    "        )\n",
    "        h_ptZ1mu2.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep2.pt,\n",
    "        )\n",
    "        return {\n",
    "            \"nMuons\": h_nMuons,\n",
    "            \"mass\": h_m4mu,\n",
    "            \"mass_z1\": h_mZ1,\n",
    "            \"mass_z2\": h_mZ2,\n",
    "            \"pt_z1_mu1\": h_ptZ1mu1,\n",
    "            \"pt_z1_mu2\": h_ptZ1mu2,\n",
    "            \"cutflow\": {dataset: cutflow},\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "We now use `apply_to_fileset` to apply our processor to the entire fileset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compute = apply_to_fileset(\n",
    "    FancyDimuonProcessor(),\n",
    "    dataset_runnable,\n",
    "    schemaclass=BaseSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "This took our fileset, and applied the processor to every dataset and returned a similar dictionary to the fileset where for every dataset, we have the output of the processor. Let's do some inspections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dak.necessary_columns(to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(to_compute, filename=\"unoptimized.pdf\", optimize_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(to_compute, filename=\"optimized.pdf\", optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "To run this processor over the whole fileset and compute the result you wouldn't want to run locally. In dask, we normally used a `Client`. We're not going to spawn a Client that will use a local cluster on our machine. In a real analysis example, the cluster could be for example a condor, slurm or other cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "(out,) = dask.compute(to_compute)\n",
    "print(out)\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nevt = (\n",
    "#     out[\"ZZto4mu\"][\"cutflow\"][\"ZZto4mu\"][\"all events\"]\n",
    "#     + out[\"DoubleMuon\"][\"cutflow\"][\"DoubleMuon\"][\"all events\"]\n",
    "# )\n",
    "# print(\"Events/s:\", (nevt / elapsed))\n",
    "\n",
    "# scale ZZ simulation to expected yield\n",
    "# lumi = 11.6  # 1/fb\n",
    "# zzxs = 7200 * 0.0336**2  # approximate 8 TeV ZZ(4mu)\n",
    "# nzz = out[\"ZZto4mu\"][\"cutflow\"][\"ZZto4mu\"][\"all events\"]\n",
    "\n",
    "# scaled = {}\n",
    "# for (name1, h1), (name2, h2) in zip(out[\"ZZto4muu\"].items(), out[\"SMHiggsToZZTo4L\"].items()):\n",
    "#     if isinstance(h1, hist.Hist) and isinstance(h2, hist.Hist):\n",
    "#         scaled[name1] = h1.copy() + h2.copy()\n",
    "#         scaled[name1].view()[0, :] *= lumi * zzxs / nzz\n",
    "\n",
    "nevt = (\n",
    "    out[\"ZZto4mu\"][\"cutflow\"][\"ZZto4mu\"][\"all events\"]\n",
    "    + out[\"SMHiggsToZZTo4L\"][\"cutflow\"][\"SMHiggsToZZTo4L\"][\"all events\"]\n",
    ")\n",
    "print(\"Events/s:\", (nevt / elapsed))\n",
    "\n",
    "scaled = {}\n",
    "for (name1, h1), (name2, h2) in zip(\n",
    "    out[\"ZZto4mu\"].items(), out[\"SMHiggsToZZTo4L\"].items()\n",
    "):\n",
    "    if isinstance(h1, hist.Hist) and isinstance(h2, hist.Hist):\n",
    "        scaled[name1] = h1.copy() + h2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Let's start looking at the results of our processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scaled[\"nMuons\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"mass\"][:, :: hist.rebin(4)].plot1d(ax=ax, overlay=\"dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"mass_z1\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"mass_z2\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "ax.set_xlim(2, 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"pt_z1_mu1\"].plot1d(ax=ax, overlay=\"dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled[\"pt_z1_mu2\"].plot1d(ax=ax, overlay=\"dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "So it's not very hard to convert an analysis from `coffea 0.7` to `coffea 202x`.\n",
    "- Processors are not needed anymore but they are a nice organization tool.\n",
    "- Coffea executors have been removed and dask takes care of that.\n",
    "- Accumulators are also not a thing anymore.\n",
    "- Awkward operations on dask-awkward arrays are automatically dispatched `ak.some_function` -> `dak.some_function`.\n",
    "- Use `import dask_awkward as dak` if you wanna be more explicit and not run operations on eager arrays by accident.\n",
    "- For histogramming, use `import hist.dask as hda`. `hist.dask` histograms behave like empty `hist` histograms, and will return a filled `hist.hist.Hist` when `.compute()` is called.\n",
    "- You'll still need to `import hist` for convenient definition of axes.\n",
    "- If you want to get a complete array or histogram object in memory on your local machine so that you can manipulate it use `array.compute()`, `ahistogram.compute()`, or `dask.compute({\"some\": array, \"another\": histogram})`\n",
    "- This should often be done at the end of code that is constructing arrays, do not prematurely `.compute()` as it can drastically slow down your analysis code.\n",
    "- Use dask clusters and clients to scale your analysis.\n",
    "- Most array operations that are available in `awkward` are available in `dask_awkward`, and if you encounter a problem or missing piece of functionality that you need you should open an issue at the [dask-awkward github page](https://github.com/dask-contrib/dask-awkward).\n",
    "- Functions that cannot be written in pure `awkward` must be wrapped and used in `dak.map_partitions`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
